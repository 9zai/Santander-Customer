3.21
1. try balanced sample
2. parameter tunning for xgb and lgb
3. models ensemble // https://www.kaggle.com/roydatascience/fork-of-eda-pca-simple-lgbm-kfold

#lr performed better in testing but not on board, casual tuning

3.25
done smte, time consuming ## and not correct
smte currently have huge effect on the model perfomance, waiting for verification

1. consist the balanced data, sample when training to save time
2. train multiple models to avg ,use different source and distribution of data
3. lgb's performances .. the kernel

3.26
smote may not work, need more attempt.

1. find out the data augement
2. tuninng param
3. combine models

kernel:
idea of data augment: https://www.kaggle.com/jiweiliu/lgb-2-leaves-augment

done test tuning the lgb
go to 0.877
need to find an augmentation method to tune the data


3.27
personal implement smote not working but imblearn seems works
left to explore

https://imbalanced-learn.org/en/stable/api.html

done over_sampling and ten rounds means, just like the validation, the lb is around 0.888
need to break through

3.28
1. no scaling did performs better
2. Maybe needs more FE 

Some interesting kernel:
https://www.kaggle.com/jesucristo/30-lines-starter-solution-fast
https://www.kaggle.com/artgor/santander-eda-fe-fs-and-models
https://www.kaggle.com/brandenkmurray/randomly-shuffled-data-also-works
https://www.kaggle.com/felipemello/boosting-creativity-towards-feature-engineering