3.21
1. try balanced sample
2. parameter tunning for xgb and lgb
3. models ensemble // https://www.kaggle.com/roydatascience/fork-of-eda-pca-simple-lgbm-kfold

#lr performed better in testing but not on board, casual tuning

3.25
done smte, time consuming ## and not correct
smte currently have huge effect on the model perfomance, waiting for verification

1. consist the balanced data, sample when training to save time
2. train multiple models to avg ,use different source and distribution of data
3. lgb's performances .. the kernel

3.26
smote may not work, need more attempt.

1. find out the data augement
2. tuninng param
3. combine models

kernel:
idea of data augment: https://www.kaggle.com/jiweiliu/lgb-2-leaves-augment

done test tuning the lgb
go to 0.877
need to find an augmentation method to tune the data


3.27
personal implement smote not working but imblearn seems works
left to explore

https://imbalanced-learn.org/en/stable/api.html

done over_sampling and ten rounds means, just like the validation, the lb is around 0.888
need to break through

3.28
1. no scaling did performs better
2. Maybe needs more FE 

Some article about lab
https://blog.csdn.net/hqr20627/article/details/79426031
https://blog.csdn.net/qq_24519677/article/details/82811215
https://www.cnblogs.com/jiangxinyang/p/9337094.html

Some interesting kernel:
https://www.kaggle.com/jesucristo/30-lines-starter-solution-fast
https://www.kaggle.com/artgor/santander-eda-fe-fs-and-models
https://www.kaggle.com/brandenkmurray/randomly-shuffled-data-also-works
https://www.kaggle.com/felipemello/boosting-creativity-towards-feature-engineering

achieve 0.898 on lp same score as local cv..
try to achieve 0.902

3.29
1. shuffle value within the same class since not effect or even improve the score -- worthy trying for augementation
https://www.kaggle.com/brandenkmurray/randomly-shuffled-data-also-works

4.2
achieve 0.900 on lb
look into the discussion about the so called 'magic'

keep tuning and I think the kde will make some sense

1. categorical some var

4.8
sight & implement
try the kernels idea
https://www.kaggle.com/c/santander-customer-transaction-prediction/discussion/87118#502797
https://www.kaggle.com/pracas/fast-pdf-correlation-matrix-how-many-groups
https://www.kaggle.com/sibmike/are-vars-mixed-up-time-intervals
https://www.kaggle.com/cdeotte/modified-naive-bayes-santander-0-899/notebook
https://www.kaggle.com/jiweiliu/fast-pdf-calculation-with-correlation-matrix

https://www.kaggle.com/c/santander-customer-transaction-prediction/discussion/87815#latest-509826
try select some features and apply linear transformation to create new features

4.10
1. change the feature fraction lower, maybe 0.15(0.2 - 0.1) not working
something more
https://www.kaggle.com/felipemello/why-your-model-is-overfitting-not-making-progress/comments
https://www.kaggle.com/c/santander-customer-transaction-prediction/discussion/87486#latest-506429
https://www.kaggle.com/cdeotte/modified-naive-bayes-santander-0-899
https://www.kaggle.com/c/santander-customer-transaction-prediction/discussion/87133#502814
